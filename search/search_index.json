{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Build AI-powered semantic search applications txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications. Traditional search systems use keywords to find data. Semantic search applications have an understanding of natural language and identify results that have the same meaning, not necessarily the same keywords. Backed by state-of-the-art machine learning models, data is transformed into vector representations for search (also known as embeddings). Innovation is happening at a rapid pace, models can understand concepts in documents, audio, images and more. Summary of txtai features: \ud83d\udd0e Large-scale similarity search with multiple index backends ( Faiss , Annoy , Hnswlib ) \ud83d\udcc4 Create embeddings for text snippets, documents, audio, images and video. Supports transformers and word vectors. \ud83d\udca1 Machine-learning pipelines to run extractive question-answering, zero-shot labeling, transcription, translation, summarization and text extraction \u21aa\ufe0f\ufe0f Workflows that join pipelines together to aggregate business logic. txtai processes can be microservices or full-fledged indexing workflows. \ud83d\udd17 API bindings for JavaScript , Java , Rust and Go \u2601\ufe0f Cloud-native architecture that scales out with container orchestration systems (e.g. Kubernetes) Applications range from similarity search to complex NLP-driven data extractions to generate structured databases. The following applications are powered by txtai. Application Description paperai AI-powered literature discovery and review engine for medical/scientific papers tldrstory AI-powered understanding of headlines and story text neuspo Fact-driven, real-time sports event and news site codequestion Ask coding questions directly from the terminal txtai is built with Python 3.7+, Hugging Face Transformers , Sentence Transformers and FastAPI","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"api/","text":"API txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality. It is suggested that separate processes are used for each instance of a txtai component. Components can be joined together with workflows. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details. Docker A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine! Distributed embeddings clusters The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index. Differences between Python and API The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. For example, any Python callable method is available at a named endpoint (i.e. instead of summary() the method call would be summary.summary()). Return types vary as tuples are returned as objects via the API. Supported language bindings The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings! application FastAPI application module apirouters () Lists available APIRouters. Returns: Type Description {router name router} Source code in txtai/api/application.py def apirouters (): \"\"\" Lists available APIRouters. Returns: {router name: router} \"\"\" # Get handle to api module api = sys . modules [ \".\" . join ( __name__ . split ( \".\" )[: - 1 ])] available = {} for name , rclass in inspect . getmembers ( api , inspect . ismodule ): if hasattr ( rclass , \"router\" ) and isinstance ( rclass . router , APIRouter ): available [ name . lower ()] = rclass . router return available get () Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" return INSTANCE start () FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings config = API . read ( os . getenv ( \"CONFIG\" )) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Get all known routers routers = apirouters () # Conditionally add routes based on configuration for name , router in routers . items (): if name in config : app . include_router ( router ) # Special case for embeddings clusters if \"cluster\" in config and \"embeddings\" not in config : app . include_router ( routers [ \"embeddings\" ]) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( routers [ \"similarity\" ]) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"API"},{"location":"api/#api","text":"txtai has a full-featured API that can optionally be enabled for any txtai process. All functionality found in txtai can be accessed via the API. The following is an example configuration and startup script for the API. Note that this configuration file enables all functionality. It is suggested that separate processes are used for each instance of a txtai component. Components can be joined together with workflows. # Index file path path : /tmp/index # Allow indexing of documents writable : True # Enbeddings index embeddings : path : sentence-transformers/nli-mpnet-base-v2 # Extractive QA extractor : path : distilbert-base-cased-distilled-squad # Zero-shot labeling labels : # Similarity similarity : # Text segmentation segmentation : sentences : true # Text summarization summary : # Text extraction textractor : paragraphs : true minlength : 100 join : true # Transcribe audio to text transcription : # Translate text between languages translation : # Workflow definitions workflow : sumfrench : tasks : - action : textractor task : storage ids : false - action : summary - action : translation args : [ \"fr\" ] sumspanish : tasks : - action : textractor task : url - action : summary - action : translation args : [ \"es\" ] Assuming this YAML content is stored in a file named index.yml, the following command starts the API process. CONFIG=index.yml uvicorn \"txtai.api:app\" uvicorn is a full-featured production ready server with support for SSL and more. See the uvicorn deployment guide for details.","title":"API"},{"location":"api/#docker","text":"A Dockerfile with commands to install txtai, all dependencies and default configuration is available in this repository. The Dockerfile can be copied from the docker directory on GitHub locally. The following commands show how to run the API process. docker build -t txtai.api -f docker/api.Dockerfile . docker run --name txtai.api -p 8000 :8000 --rm -it txtai.api # Alternatively, if nvidia-docker is installed, the build will support GPU runtimes docker run --name txtai.api --runtime = nvidia -p 8000 :8000 --rm -it txtai.api This will bring up an API instance without having to install Python, txtai or any dependencies on your machine!","title":"Docker"},{"location":"api/#distributed-embeddings-clusters","text":"The API supports combining multiple API instances into a single logical embeddings index. An example configuration is shown below. cluster : shards : - http://127.0.0.1:8002 - http://127.0.0.1:8003 This configuration aggregates the API instances above as index shards. Data is evenly split among each of the shards at index time. Queries are run in parallel against each shard and the results are joined together. This method allows horizontal scaling and supports very large index clusters. This method is only recommended for data sets in the 1 billion+ records. The ANN libraries can easily support smaller data sizes and this method is not worth the additional complexity. At this time, new shards can not be added after building the initial index.","title":"Distributed embeddings clusters"},{"location":"api/#differences-between-python-and-api","text":"The txtai API provides all the major functionality found in this project. But there are differences due to the nature of JSON and differences across the supported programming languages. For example, any Python callable method is available at a named endpoint (i.e. instead of summary() the method call would be summary.summary()). Return types vary as tuples are returned as objects via the API.","title":"Differences between Python and API"},{"location":"api/#supported-language-bindings","text":"The following programming languages have txtai bindings: JavaScript Java Rust Go See each of the projects above for details on how to install and use. Please add an issue to request additional language bindings!","title":"Supported language bindings"},{"location":"api/#txtai.api.application","text":"FastAPI application module","title":"application"},{"location":"api/#txtai.api.application.apirouters","text":"Lists available APIRouters. Returns: Type Description {router name router} Source code in txtai/api/application.py def apirouters (): \"\"\" Lists available APIRouters. Returns: {router name: router} \"\"\" # Get handle to api module api = sys . modules [ \".\" . join ( __name__ . split ( \".\" )[: - 1 ])] available = {} for name , rclass in inspect . getmembers ( api , inspect . ismodule ): if hasattr ( rclass , \"router\" ) and isinstance ( rclass . router , APIRouter ): available [ name . lower ()] = rclass . router return available","title":"apirouters()"},{"location":"api/#txtai.api.application.get","text":"Returns global API instance. Returns: Type Description API instance Source code in txtai/api/application.py def get (): \"\"\" Returns global API instance. Returns: API instance \"\"\" return INSTANCE","title":"get()"},{"location":"api/#txtai.api.application.start","text":"FastAPI startup event. Loads API instance. Source code in txtai/api/application.py @app . on_event ( \"startup\" ) def start (): \"\"\" FastAPI startup event. Loads API instance. \"\"\" # pylint: disable=W0603 global INSTANCE # Load YAML settings config = API . read ( os . getenv ( \"CONFIG\" )) # Instantiate API instance api = os . getenv ( \"API_CLASS\" ) INSTANCE = Factory . create ( config , api ) if api else API ( config ) # Get all known routers routers = apirouters () # Conditionally add routes based on configuration for name , router in routers . items (): if name in config : app . include_router ( router ) # Special case for embeddings clusters if \"cluster\" in config and \"embeddings\" not in config : app . include_router ( routers [ \"embeddings\" ]) # Special case to add similarity instance for embeddings if \"embeddings\" in config and \"similarity\" not in config : app . include_router ( routers [ \"similarity\" ]) # Execute extensions if present extensions = os . getenv ( \"EXTENSIONS\" ) if extensions : for extension in extensions . split ( \",\" ): # Create instance and execute extension extension = Factory . get ( extension . strip ())() extension ( app )","title":"start()"},{"location":"articles/","text":"Further reading Introducing txtai, AI-powered semantic search built on Transformers Run machine-learning workflows to transform data and build AI-powered semantic search applications with txtai Semantic search on the cheap Tutorial series on dev.to","title":"Further Reading"},{"location":"articles/#further-reading","text":"Introducing txtai, AI-powered semantic search built on Transformers Run machine-learning workflows to transform data and build AI-powered semantic search applications with txtai Semantic search on the cheap Tutorial series on dev.to","title":"Further reading"},{"location":"embeddings/","text":"Embeddings Embeddings is the engine that delivers semantic search. Text is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used find results that have the same meaning, not necessarily the same keywords. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Word embeddings model Embeddings ({ \"method\" : \"words\" , \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True }) Configuration method method : transformers|sentence-transformers|words Sentence embeddings method to use. Options listed below. transformers Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings. sentence-transformers Same as transformers but loads models with the sentence-transformers library. words Builds sentence embeddings using a word embeddings model. sentence-transformers and words require the similarity extras package to be installed. The method is inferred using the path if not provided. path path : string Required field that sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model. backend backend : faiss|hnsw|annoy Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted. faiss faiss : components : Comma separated list of components - defaults to \"Flat\" for small indices and \"IVFx,Flat\" for larger indexes where x = 4 * sqrt(embeddings count) nprobe : search probe setting (int) - defaults to x/16 (as defined above) for larger indexes See the following Faiss documentation links for more information. Guidelines for choosing an index Index configuration summary Index Factory Search Tuning hnsw hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters. annoy annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported. quantize quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization. Additional configuration for Transformers models tokenize tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations. Additional configuration for Word embedding models Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed. storevectors storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies. scoring scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built. pca pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied. __init__ ( self , config = None ) special Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms text into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config ) batchsearch ( self , queries , limit = 3 ) Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query for ann search, list of dict per query for an ann+database search Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit ) batchsimilarity ( self , queries , texts ) Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ] batchtransform ( self , documents ) Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ] index ( self , documents , reindex = False ) Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, dict|text|tokens, tags) required reindex if this is a reindex operation in which case database creation is skipped, defaults to False False Source code in txtai/embeddings/base.py def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, dict|text|tokens, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Transform documents to embeddings vectors ids , dimensions , embeddings = self . vectors ( documents ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Build the index self . ann . index ( embeddings ) # Keep existing database and archive instances if this is part of a reindex if not reindex : # Create document database self . database = self . createdatabase () if self . database : # Add documents to database self . database . insert ( documents ) else : # Save indexids-ids mapping for indexes with no database self . config [ \"ids\" ] = ids # Reset archive since this is a new index self . archive = None load ( self , path ) Loads an existing index from path. Parameters: Name Type Description Default path input path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms text to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" ) save ( self , path ) Saves an index. Parameters: Name Type Description Default path output path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath ) score ( self , documents ) Builds a scoring index. Only used by word vectors models. Parameters: Name Type Description Default documents list of (id, dict|text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, dict|text|tokens, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents ) search ( self , query , limit = 3 ) Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) for ann search, list of dict for an ann+database search Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" return self . batchsearch ([ query ], limit )[ 0 ] similarity ( self , query , texts ) Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ] transform ( self , document ) Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"Embeddings"},{"location":"embeddings/#embeddings","text":"Embeddings is the engine that delivers semantic search. Text is transformed into embeddings vectors where similar concepts will produce similar vectors. Indexes both large and small are built with these vectors. The indexes are used find results that have the same meaning, not necessarily the same keywords. Embeddings parameters are set through the constructor. Examples below. # Transformers embeddings model Embeddings ({ \"method\" : \"transformers\" , \"path\" : \"sentence-transformers/nli-mpnet-base-v2\" }) # Word embeddings model Embeddings ({ \"method\" : \"words\" , \"path\" : vectors , \"storevectors\" : True , \"scoring\" : \"bm25\" , \"pca\" : 3 , \"quantize\" : True })","title":"Embeddings"},{"location":"embeddings/#configuration","text":"","title":"Configuration"},{"location":"embeddings/#method","text":"method : transformers|sentence-transformers|words Sentence embeddings method to use. Options listed below.","title":"method"},{"location":"embeddings/#transformers","text":"Builds sentence embeddings using a transformers model. While this can be any transformers model, it works best with models trained to build sentence embeddings.","title":"transformers"},{"location":"embeddings/#sentence-transformers","text":"Same as transformers but loads models with the sentence-transformers library.","title":"sentence-transformers"},{"location":"embeddings/#words","text":"Builds sentence embeddings using a word embeddings model. sentence-transformers and words require the similarity extras package to be installed. The method is inferred using the path if not provided.","title":"words"},{"location":"embeddings/#path","text":"path : string Required field that sets the path for a vectors model. When using a transformers/sentence-transformers model, this can be any model on the Hugging Face Model Hub or a local file path. Otherwise, it must be a local file path to a word embeddings model.","title":"path"},{"location":"embeddings/#backend","text":"backend : faiss|hnsw|annoy Approximate Nearest Neighbor (ANN) index backend for storing generated sentence embeddings. Defaults to Faiss. Additional backends require the similarity extras package to be installed. Backend-specific settings are set with a corresponding configuration object having the same name as the backend (i.e. annoy, faiss, or hnsw). None of these are required and are set to defaults if omitted.","title":"backend"},{"location":"embeddings/#faiss","text":"faiss : components : Comma separated list of components - defaults to \"Flat\" for small indices and \"IVFx,Flat\" for larger indexes where x = 4 * sqrt(embeddings count) nprobe : search probe setting (int) - defaults to x/16 (as defined above) for larger indexes See the following Faiss documentation links for more information. Guidelines for choosing an index Index configuration summary Index Factory Search Tuning","title":"faiss"},{"location":"embeddings/#hnsw","text":"hnsw : efconstruction : ef_construction param for init_index (int) - defaults to 200 m : M param for init_index (int) - defaults to 16 randomseed : random-seed param for init_index (init) - defaults to 100 efsearch : ef search param (int) - defaults to None and not set See Hnswlib documentation for more information on these parameters.","title":"hnsw"},{"location":"embeddings/#annoy","text":"annoy : ntrees : number of trees (int) - defaults to 10 searchk : search_k search setting (int) - defaults to -1 See Annoy documentation for more information on these parameters. Note that annoy indexes can not be modified after creation, upserts/deletes and other modifications are not supported.","title":"annoy"},{"location":"embeddings/#quantize","text":"quantize : boolean Enables quanitization of generated sentence embeddings. If the index backend supports it, sentence embeddings will be stored with 8-bit precision vs 32-bit. Only Faiss currently supports quantization.","title":"quantize"},{"location":"embeddings/#additional-configuration-for-transformers-models","text":"","title":"Additional configuration for Transformers models"},{"location":"embeddings/#tokenize","text":"tokenize : boolean Enables string tokenization (defaults to false). This method applies tokenization rules that only work with English language text and may increase the quality of English language sentence embeddings in some situations.","title":"tokenize"},{"location":"embeddings/#additional-configuration-for-word-embedding-models","text":"Word embeddings provide a good tradeoff of performance to functionality for a similarity search system. With that being said, Transformers models are making great progress in scaling performance down to smaller models and are the preferred vector backend in txtai for most cases. Word embeddings models require the similarity extras package to be installed.","title":"Additional configuration for Word embedding models"},{"location":"embeddings/#storevectors","text":"storevectors : boolean Enables copying of a vectors model set in path into the embeddings models output directory on save. This option enables a fully encapsulated index with no external file dependencies.","title":"storevectors"},{"location":"embeddings/#scoring","text":"scoring : bm25|tfidf|sif A scoring model builds weighted averages of word vectors for a given sentence. Supports BM25, TF-IDF and SIF (smooth inverse frequency) methods. If a scoring method is not provided, mean sentence embeddings are built.","title":"scoring"},{"location":"embeddings/#pca","text":"pca : int Removes n principal components from generated sentence embeddings. When enabled, a TruncatedSVD model is built to help with dimensionality reduction. After pooling of vectors creates a single sentence embedding, this method is applied.","title":"pca"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.__init__","text":"Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Parameters: Name Type Description Default config embeddings configuration None Source code in txtai/embeddings/base.py def __init__ ( self , config = None ): \"\"\" Creates a new embeddings index. Embeddings indexes are thread-safe for read operations but writes must be synchronized. Args: config: embeddings configuration \"\"\" # Index configuration self . config = None # Dimensionality reduction and scoring models - word vectors only self . reducer , self . scoring = None , None # Embeddings vector model - transforms text into similarity vectors self . model = None # Approximate nearest neighbor index self . ann = None # Document database self . database = None # Index archive self . archive = None # Set initial configuration self . configure ( config )","title":"__init__()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsearch","text":"Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default queries queries text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) per query for ann search, list of dict per query for an ann+database search Source code in txtai/embeddings/base.py def batchsearch ( self , queries , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: queries: queries text|tokens limit: maximum results Returns: list of (id, score) per query for ann search, list of dict per query for an ann+database search \"\"\" return Search ( self )( queries , limit )","title":"batchsearch()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchsimilarity","text":"Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Parameters: Name Type Description Default queries queries text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) per query Source code in txtai/embeddings/base.py def batchsimilarity ( self , queries , texts ): \"\"\" Computes the similarity between list of queries and list of text. Returns a list of (id, score) sorted by highest score per query, where id is the index in texts. Args: queries: queries text|tokens texts: list of text|tokens Returns: list of (id, score) per query \"\"\" # Convert queries to embedding vectors queries = np . array ([ self . transform (( None , query , None )) for query in queries ]) texts = np . array ([ self . transform (( None , text , None )) for text in texts ]) # Dot product on normalized vectors is equal to cosine similarity scores = np . dot ( queries , texts . T ) . tolist () # Add index and sort desc based on score return [ sorted ( enumerate ( score ), key = lambda x : x [ 1 ], reverse = True ) for score in scores ]","title":"batchsimilarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.batchtransform","text":"Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default documents list of (id, text|tokens, tags) required Returns: Type Description embeddings vectors Source code in txtai/embeddings/base.py def batchtransform ( self , documents ): \"\"\" Transforms documents into embeddings vectors. Document text will be tokenized if not pre-tokenized. Args: documents: list of (id, text|tokens, tags) Returns: embeddings vectors \"\"\" return [ self . transform ( document ) for document in documents ]","title":"batchtransform()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.index","text":"Builds an embeddings index. This method overwrites an existing index. Parameters: Name Type Description Default documents list of (id, dict|text|tokens, tags) required reindex if this is a reindex operation in which case database creation is skipped, defaults to False False Source code in txtai/embeddings/base.py def index ( self , documents , reindex = False ): \"\"\" Builds an embeddings index. This method overwrites an existing index. Args: documents: list of (id, dict|text|tokens, tags) reindex: if this is a reindex operation in which case database creation is skipped, defaults to False \"\"\" # Transform documents to embeddings vectors ids , dimensions , embeddings = self . vectors ( documents ) # Build LSA model (if enabled). Remove principal components from embeddings. if self . config . get ( \"pca\" ): self . reducer = Reducer ( embeddings , self . config [ \"pca\" ]) self . reducer ( embeddings ) # Normalize embeddings self . normalize ( embeddings ) # Save index dimensions self . config [ \"dimensions\" ] = dimensions # Create approximate nearest neighbor index self . ann = ANNFactory . create ( self . config ) # Build the index self . ann . index ( embeddings ) # Keep existing database and archive instances if this is part of a reindex if not reindex : # Create document database self . database = self . createdatabase () if self . database : # Add documents to database self . database . insert ( documents ) else : # Save indexids-ids mapping for indexes with no database self . config [ \"ids\" ] = ids # Reset archive since this is a new index self . archive = None","title":"index()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.load","text":"Loads an existing index from path. Parameters: Name Type Description Default path input path required Source code in txtai/embeddings/base.py def load ( self , path ): \"\"\" Loads an existing index from path. Args: path: input path \"\"\" # Check if this is an archive file and extract path , apath = self . checkarchive ( path ) if apath : self . archive . load ( apath ) # Index configuration with open ( f \" { path } /config\" , \"rb\" ) as handle : self . config = pickle . load ( handle ) # Build full path to embedding vectors file if self . config . get ( \"storevectors\" ): self . config [ \"path\" ] = os . path . join ( path , self . config [ \"path\" ]) # Approximate nearest neighbor index - stores embeddings vectors self . ann = ANNFactory . create ( self . config ) self . ann . load ( f \" { path } /embeddings\" ) # Dimensionality reduction model - word vectors only if self . config . get ( \"pca\" ): self . reducer = Reducer () self . reducer . load ( f \" { path } /lsa\" ) # Embedding scoring model - word vectors only if self . config . get ( \"scoring\" ): self . scoring = ScoringFactory . create ( self . config [ \"scoring\" ]) self . scoring . load ( f \" { path } /scoring\" ) # Sentence vectors model - transforms text to embeddings vectors self . model = self . loadvectors () # Document database - stores document content self . database = self . createdatabase () if self . database : self . database . load ( f \" { path } /documents\" )","title":"load()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.save","text":"Saves an index. Parameters: Name Type Description Default path output path required Source code in txtai/embeddings/base.py def save ( self , path ): \"\"\" Saves an index. Args: path: output path \"\"\" if self . config : # Check if this is an archive file path , apath = self . checkarchive ( path ) # Create output directory, if necessary os . makedirs ( path , exist_ok = True ) # Copy sentence vectors model if self . config . get ( \"storevectors\" ): shutil . copyfile ( self . config [ \"path\" ], os . path . join ( path , os . path . basename ( self . config [ \"path\" ]))) self . config [ \"path\" ] = os . path . basename ( self . config [ \"path\" ]) # Write index configuration with open ( f \" { path } /config\" , \"wb\" ) as handle : pickle . dump ( self . config , handle , protocol = pickle . HIGHEST_PROTOCOL ) # Save approximate nearest neighbor index self . ann . save ( f \" { path } /embeddings\" ) # Save dimensionality reduction model (word vectors only) if self . reducer : self . reducer . save ( f \" { path } /lsa\" ) # Save embedding scoring model (word vectors only) if self . scoring : self . scoring . save ( f \" { path } /scoring\" ) # Save document database if self . database : self . database . save ( f \" { path } /documents\" ) # If this is an archive, save it if apath : self . archive . save ( apath )","title":"save()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.score","text":"Builds a scoring index. Only used by word vectors models. Parameters: Name Type Description Default documents list of (id, dict|text|tokens, tags) required Source code in txtai/embeddings/base.py def score ( self , documents ): \"\"\" Builds a scoring index. Only used by word vectors models. Args: documents: list of (id, dict|text|tokens, tags) \"\"\" # Build scoring index over documents if self . scoring : self . scoring . index ( documents )","title":"score()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.search","text":"Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Parameters: Name Type Description Default query query text|tokens required limit maximum results 3 Returns: Type Description list of (id, score) for ann search, list of dict for an ann+database search Source code in txtai/embeddings/base.py def search ( self , query , limit = 3 ): \"\"\" Finds documents most similar to the input queries. This method will run either an approximate nearest neighbor (ann) search or an approximate nearest neighbor + database search depending on if a database is available. Args: query: query text|tokens limit: maximum results Returns: list of (id, score) for ann search, list of dict for an ann+database search \"\"\" return self . batchsearch ([ query ], limit )[ 0 ]","title":"search()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.similarity","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Parameters: Name Type Description Default query query text|tokens required texts list of text|tokens required Returns: Type Description list of (id, score) Source code in txtai/embeddings/base.py def similarity ( self , query , texts ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. Args: query: query text|tokens texts: list of text|tokens Returns: list of (id, score) \"\"\" return self . batchsimilarity ([ query ], texts )[ 0 ]","title":"similarity()"},{"location":"embeddings/#txtai.embeddings.base.Embeddings.transform","text":"Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Parameters: Name Type Description Default document (id, text|tokens, tags) required Returns: Type Description embeddings vector Source code in txtai/embeddings/base.py def transform ( self , document ): \"\"\" Transforms document into an embeddings vector. Document text will be tokenized if not pre-tokenized. Args: document: (id, text|tokens, tags) Returns: embeddings vector \"\"\" # Convert document into sentence embedding embedding = self . model . transform ( document ) # Reduce the dimensionality of the embeddings. Scale the embeddings using this # model to reduce the noise of common but less relevant terms. if self . reducer : self . reducer ( embedding ) # Normalize embeddings self . normalize ( embedding ) return embedding","title":"transform()"},{"location":"examples/","text":"Examples The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below. Semantic Search Build semantic/similarity/vector search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems API Gallery Using txtai in JavaScript, Java, Rust and Go Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes Pipelines Transform data with NLP-backed pipelines. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection Workflows Efficiently process data at scale. Notebook Description Run pipeline workflows Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays Model Training Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more Applications Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces also provided. Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17","title":"Examples"},{"location":"examples/#examples","text":"The examples directory has a series of notebooks and applications giving an overview of txtai. See the sections below.","title":"Examples"},{"location":"examples/#semantic-search","text":"Build semantic/similarity/vector search applications. Notebook Description Introducing txtai Overview of the functionality provided by txtai Build an Embeddings index with Hugging Face Datasets Index and search Hugging Face Datasets Build an Embeddings index from a data source Index and search a data source with word embeddings Add semantic search to Elasticsearch Add semantic search to existing search systems API Gallery Using txtai in JavaScript, Java, Rust and Go Similarity search with images Embed images and text into the same space for search Distributed embeddings cluster Distribute an embeddings index across multiple data nodes","title":"Semantic Search"},{"location":"examples/#pipelines","text":"Transform data with NLP-backed pipelines. Notebook Description Extractive QA with txtai Introduction to extractive question-answering with txtai Extractive QA with Elasticsearch Run extractive question-answering queries with Elasticsearch Extractive QA to build structured data Build structured datasets using extractive question-answering Apply labels with zero shot classification Use zero shot learning for labeling, classification and topic modeling Building abstractive text summaries Run abstractive text summarization Extract text from documents Extract text from PDF, Office, HTML and more Transcribe audio to text Convert audio files to text Translate text between languages Streamline machine translation and language detection","title":"Pipelines"},{"location":"examples/#workflows","text":"Efficiently process data at scale. Notebook Description Run pipeline workflows Simple yet powerful constructs to efficiently process data Transform tabular data with composable workflows Transform, index and search tabular data Tensor workflows Performant processing of large tensor arrays","title":"Workflows"},{"location":"examples/#model-training","text":"Train NLP models. Notebook Description Train a text labeler Build text sequence classification models Train without labels Use zero-shot classifiers to train new models Train a QA model Build and fine-tune question-answering models Export and run models with ONNX Export models with ONNX, run natively in JavaScript, Java and Rust Export and run other machine learning models Export and run models from scikit-learn, PyTorch and more","title":"Model Training"},{"location":"examples/#applications","text":"Series of example applications with txtai. Links to hosted versions on Hugging Face Spaces also provided. Application Description Basic similarity search Basic similarity search example. Data from the original txtai demo. \ud83e\udd17 Book search Book similarity search application. Index book descriptions and query using natural language statements. Local run only Image search Image similarity search application. Index a directory of images and run searches to identify images similar to the input query. \ud83e\udd17 Summarize an article Summarize an article. Workflow that extracts text from a webpage and builds a summary. \ud83e\udd17 Wiki search Wikipedia search application. Queries Wikipedia API and summarizes the top result. \ud83e\udd17 Workflow builder Build and execute txtai workflows. Connect summarization, text extraction, transcription, translation and similarity search pipelines together to run unified workflows. \ud83e\udd17","title":"Applications"},{"location":"install/","text":"Installation The easiest way to install is via pip and PyPI pip install txtai Python 3.7+ is supported. Using a Python virtual environment is recommended. Install from source txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai Environment specific prerequisites Additional environment specific prerequisites are below. Linux Optional audio transcription requires a system library to be installed macOS Run brew install libomp see this link Windows Optional dependencies require C++ Build Tools Optional dependencies txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections. All Install all dependencies (mirrors txtai < 3.2) pip install txtai[all] API Serve txtai via a web API. pip install txtai[api] Pipeline All pipelines - default install comes with most common pipelines. pip install txtai[pipeline] Similarity Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity] Workflow All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Installation"},{"location":"install/#installation","text":"The easiest way to install is via pip and PyPI pip install txtai Python 3.7+ is supported. Using a Python virtual environment is recommended.","title":"Installation"},{"location":"install/#install-from-source","text":"txtai can also be installed directly from GitHub to access the latest, unreleased features. pip install git+https://github.com/neuml/txtai","title":"Install from source"},{"location":"install/#environment-specific-prerequisites","text":"Additional environment specific prerequisites are below.","title":"Environment specific prerequisites"},{"location":"install/#linux","text":"Optional audio transcription requires a system library to be installed","title":"Linux"},{"location":"install/#macos","text":"Run brew install libomp see this link","title":"macOS"},{"location":"install/#windows","text":"Optional dependencies require C++ Build Tools","title":"Windows"},{"location":"install/#optional-dependencies","text":"txtai has the following optional dependencies that can be installed as extras. The patterns below are supported in setup.py install_requires sections.","title":"Optional dependencies"},{"location":"install/#all","text":"Install all dependencies (mirrors txtai < 3.2) pip install txtai[all]","title":"All"},{"location":"install/#api","text":"Serve txtai via a web API. pip install txtai[api]","title":"API"},{"location":"install/#pipeline","text":"All pipelines - default install comes with most common pipelines. pip install txtai[pipeline]","title":"Pipeline"},{"location":"install/#similarity","text":"Word vectors, support for sentence-transformers models not on the HF Hub and additional ANN libraries. pip install txtai[similarity]","title":"Similarity"},{"location":"install/#workflow","text":"All workflow tasks - default install comes with most common workflow tasks. pip install txtai[workflow] Multiple dependencies can be specified at the same time. pip install txtai[pipeline,workflow]","title":"Workflow"},{"location":"why/","text":"Why txtai? In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all that is needed to get started Works well with both small and big data, prototypes can be built in a couple lines of code, scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low install footprint, most dependencies optional and only required when you need them Learn by example, notebooks cover all available functionality","title":"Why txtai?"},{"location":"why/#why-txtai","text":"In addition to traditional search systems, a growing number of semantic search solutions are available, so why txtai? pip install txtai is all that is needed to get started Works well with both small and big data, prototypes can be built in a couple lines of code, scale up as needed Rich data processing framework (pipelines and workflows) to pre and post process data Work in your programming language of choice via the API Modular with low install footprint, most dependencies optional and only required when you need them Learn by example, notebooks cover all available functionality","title":"Why txtai?"},{"location":"pipeline/overview/","text":"Pipeline txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Audio Transcription Data Processing Segmentation Tabular Text extraction Image Caption Objects Text Extractive QA Labeling Similarity Summary Translation Training HF ONNX ML ONNX Trainer","title":"Overview"},{"location":"pipeline/overview/#pipeline","text":"txtai provides a generic pipeline processing framework with the only interface requirement being a __call__ method. Pipelines are flexible and can process various types of data. Pipelines can wrap machine learning models as well as other processes. The following pipeline types are currently supported. Audio Transcription Data Processing Segmentation Tabular Text extraction Image Caption Objects Text Extractive QA Labeling Similarity Summary Translation Training HF ONNX ML ONNX Trainer","title":"Pipeline"},{"location":"pipeline/audio/transcription/","text":"Transcription A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" ) __init__ ( self , path = 'facebook/wav2vec2-base-960h' , quantize = False , gpu = True , batch = 64 ) special Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/audio/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , files ) special Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/audio/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"Transcription"},{"location":"pipeline/audio/transcription/#transcription","text":"A Transcription pipeline converts speech in audio files to text. Transcription parameters are set as constructor arguments. Examples below. Transcription () Transcription ( \"facebook/wav2vec2-large-960h\" )","title":"Transcription"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__init__","text":"Constructs a new transcription pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/wav2vec2-base-960h' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/audio/transcription.py def __init__ ( self , path = \"facebook/wav2vec2-base-960h\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new transcription pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) if not SOUNDFILE : raise ImportError ( \"SoundFile library not installed or libsndfile not found\" ) # load model and processor self . model = Wav2Vec2ForCTC . from_pretrained ( self . path ) self . processor = Wav2Vec2Processor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipeline/audio/transcription/#txtai.pipeline.audio.transcription.Transcription.__call__","text":"Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default files text|list required Returns: Type Description list of transcribed text Source code in txtai/pipeline/audio/transcription.py def __call__ ( self , files ): \"\"\" Transcribes audio files to text. This method supports files as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: files: text|list Returns: list of transcribed text \"\"\" values = [ files ] if not isinstance ( files , list ) else files # Parse audio files speech = [ sf . read ( f ) for f in values ] # Get unique list of sampling rates unique = set ( s [ 1 ] for s in speech ) results = {} for sampling in unique : # Get inputs for current sampling rate inputs = [( x , s [ 0 ]) for x , s in enumerate ( speech ) if s [ 1 ] == sampling ] # Transcribe in batches outputs = [] for chunk in self . batch ([ s for _ , s in inputs ], self . batchsize ): outputs . extend ( self . transcribe ( chunk , sampling )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] . capitalize () # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( files , str ) else results","title":"__call__()"},{"location":"pipeline/data/segmentation/","text":"Segmentation A Segmentation pipeline segments text into semantic units. Segmentation parameters are set as constructor arguments. Examples below. Segmentation () __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ) special Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/data/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join __call__ ( self , text ) special Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Segmentation"},{"location":"pipeline/data/segmentation/#segmentation","text":"A Segmentation pipeline segments text into semantic units. Segmentation parameters are set as constructor arguments. Examples below. Segmentation ()","title":"Segmentation"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__init__","text":"Creates a new Segmentation pipeline. Parameters: Name Type Description Default sentences tokenize text into sentences if True, defaults to False False lines tokenizes text into lines if True, defaults to False False paragraphs tokenizes text into paragraphs if True, defaults to False False minlength require at least minlength characters per text element, defaults to None None join joins tokenized sections back together if True, defaults to False False Source code in txtai/pipeline/data/segmentation.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False ): \"\"\" Creates a new Segmentation pipeline. Args: sentences: tokenize text into sentences if True, defaults to False lines: tokenizes text into lines if True, defaults to False paragraphs: tokenizes text into paragraphs if True, defaults to False minlength: require at least minlength characters per text element, defaults to None join: joins tokenized sections back together if True, defaults to False \"\"\" if not NLTK : raise ImportError ( 'Segmentation pipeline is not available - install \"pipeline\" extra to enable' ) self . sentences = sentences self . lines = lines self . paragraphs = paragraphs self . minlength = minlength self . join = join","title":"__init__()"},{"location":"pipeline/data/segmentation/#txtai.pipeline.data.segmentation.Segmentation.__call__","text":"Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Parameters: Name Type Description Default text text|list required Returns: Type Description segmented text Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/data/tabular/","text":"Tabular A Tabular pipeline splits tabular data into rows and columns. Tabular ( idcolumns , [ textcolumns ]) __init__ ( self , idcolumn = None , textcolumns = None , content = False ) special Creates a new Tabular pipeline. Parameters: Name Type Description Default idcolumn column name to use for row id None textcolumns list of columns to combine as a text field None content if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. False Source code in txtai/pipeline/data/tabular.py def __init__ ( self , idcolumn = None , textcolumns = None , content = False ): \"\"\" Creates a new Tabular pipeline. Args: idcolumn: column name to use for row id textcolumns: list of columns to combine as a text field content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. \"\"\" if not PANDAS : raise ImportError ( 'Tabular pipeline is not available - install \"pipeline\" extra to enable' ) self . idcolumn = idcolumn self . textcolumns = textcolumns self . content = content __call__ ( self , data ) special Splits data into rows and columns. Parameters: Name Type Description Default data input data required Returns: Type Description list of (id, text, tag) Source code in txtai/pipeline/data/tabular.py def __call__ ( self , data ): \"\"\" Splits data into rows and columns. Args: data: input data Returns: list of (id, text, tag) \"\"\" items = [ data ] if not isinstance ( data , list ) else data # Combine all rows into single return element results = [] dicts = [] for item in items : # File path if isinstance ( item , str ): _ , extension = os . path . splitext ( item ) extension = extension . replace ( \".\" , \"\" ) . lower () if extension == \"csv\" : df = pd . read_csv ( item ) results . append ( self . process ( df )) # Dict if isinstance ( item , dict ): dicts . append ( item ) # List of dicts elif isinstance ( item , list ): df = pd . DataFrame ( item ) results . append ( self . process ( df )) if dicts : df = pd . DataFrame ( dicts ) results . extend ( self . process ( df )) return results [ 0 ] if not isinstance ( data , list ) else results","title":"Tabular"},{"location":"pipeline/data/tabular/#tabular","text":"A Tabular pipeline splits tabular data into rows and columns. Tabular ( idcolumns , [ textcolumns ])","title":"Tabular"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__init__","text":"Creates a new Tabular pipeline. Parameters: Name Type Description Default idcolumn column name to use for row id None textcolumns list of columns to combine as a text field None content if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. False Source code in txtai/pipeline/data/tabular.py def __init__ ( self , idcolumn = None , textcolumns = None , content = False ): \"\"\" Creates a new Tabular pipeline. Args: idcolumn: column name to use for row id textcolumns: list of columns to combine as a text field content: if True, a dict per row is generated with all fields. If content is a list, a subset of fields is included in the generated rows. \"\"\" if not PANDAS : raise ImportError ( 'Tabular pipeline is not available - install \"pipeline\" extra to enable' ) self . idcolumn = idcolumn self . textcolumns = textcolumns self . content = content","title":"__init__()"},{"location":"pipeline/data/tabular/#txtai.pipeline.data.tabular.Tabular.__call__","text":"Splits data into rows and columns. Parameters: Name Type Description Default data input data required Returns: Type Description list of (id, text, tag) Source code in txtai/pipeline/data/tabular.py def __call__ ( self , data ): \"\"\" Splits data into rows and columns. Args: data: input data Returns: list of (id, text, tag) \"\"\" items = [ data ] if not isinstance ( data , list ) else data # Combine all rows into single return element results = [] dicts = [] for item in items : # File path if isinstance ( item , str ): _ , extension = os . path . splitext ( item ) extension = extension . replace ( \".\" , \"\" ) . lower () if extension == \"csv\" : df = pd . read_csv ( item ) results . append ( self . process ( df )) # Dict if isinstance ( item , dict ): dicts . append ( item ) # List of dicts elif isinstance ( item , list ): df = pd . DataFrame ( item ) results . append ( self . process ( df )) if dicts : df = pd . DataFrame ( dicts ) results . extend ( self . process ( df )) return results [ 0 ] if not isinstance ( data , list ) else results","title":"__call__()"},{"location":"pipeline/data/textractor/","text":"Textractor A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor () __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ) special Source code in txtai/pipeline/data/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join ) # Determine if Tika (default if Java is available) or Beautiful Soup should be used # Beautiful Soup only supports HTML, Tika supports a wide variety of file formats, including HTML. self . tika = self . checkjava () if tika else False __call__ ( self , text ) special Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"Textractor"},{"location":"pipeline/data/textractor/#textractor","text":"A Textractor pipeline extracts and splits text from documents. Textractor parameters are set as constructor arguments. Examples below. Textractor ()","title":"Textractor"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.textractor.Textractor.__init__","text":"Source code in txtai/pipeline/data/textractor.py def __init__ ( self , sentences = False , lines = False , paragraphs = False , minlength = None , join = False , tika = True ): if not TIKA : raise ImportError ( 'Textractor pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( sentences , lines , paragraphs , minlength , join ) # Determine if Tika (default if Java is available) or Beautiful Soup should be used # Beautiful Soup only supports HTML, Tika supports a wide variety of file formats, including HTML. self . tika = self . checkjava () if tika else False","title":"__init__()"},{"location":"pipeline/data/textractor/#txtai.pipeline.data.segmentation.Textractor.__call__","text":"Source code in txtai/pipeline/data/segmentation.py def __call__ ( self , text ): \"\"\" Segments text into semantic units. This method supports text as a string or a list. If the input is a string, the return type is text|list. If text is a list, a list of returned, this could be a list of text or a list of lists depending on the tokenization strategy. Args: text: text|list Returns: segmented text \"\"\" # Get inputs texts = [ text ] if not isinstance ( text , list ) else text # Extract text for each input file results = [] for value in texts : # Get text value = self . text ( value ) # Parse and add extracted results results . append ( self . parse ( value )) return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/image/caption/","text":"Caption A caption pipeline reads a list of images and returns a list of captions for those images. Caption () __init__ ( self , path = 'ydshieh/vit-gpt2-coco-en' , quantize = False , gpu = True , batch = 64 ) special Constructs a new caption pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'ydshieh/vit-gpt2-coco-en' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/image/caption.py def __init__ ( self , path = \"ydshieh/vit-gpt2-coco-en\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new caption pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" if not PIL : raise ImportError ( 'Captions pipeline is not available - install \"pipeline\" extra to enable' ) # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # load model and processor self . model = VisionEncoderDecoderModel . from_pretrained ( self . path ) self . tokenizer = AutoTokenizer . from_pretrained ( self . path ) self . extractor = ViTFeatureExtractor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device ) __call__ ( self , images ) special Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Parameters: Name Type Description Default images image|list required Returns: Type Description list of captions Source code in txtai/pipeline/image/caption.py def __call__ ( self , images ): \"\"\" Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Args: images: image|list Returns: list of captions \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Feature extraction pixels = self . extractor ( images = values , return_tensors = \"pt\" ) . pixel_values pixels = pixels . to ( self . device ) # Run model with torch . no_grad (): outputs = self . model . generate ( pixels , max_length = 16 , num_beams = 4 , return_dict_in_generate = True ) . sequences # Tokenize outputs into text results captions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) captions = [ caption . strip () for caption in captions ] # Return single element if single element passed in return captions [ 0 ] if not isinstance ( images , list ) else captions","title":"Caption"},{"location":"pipeline/image/caption/#caption","text":"A caption pipeline reads a list of images and returns a list of captions for those images. Caption ()","title":"Caption"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__init__","text":"Constructs a new caption pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'ydshieh/vit-gpt2-coco-en' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 Source code in txtai/pipeline/image/caption.py def __init__ ( self , path = \"ydshieh/vit-gpt2-coco-en\" , quantize = False , gpu = True , batch = 64 ): \"\"\" Constructs a new caption pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content \"\"\" if not PIL : raise ImportError ( 'Captions pipeline is not available - install \"pipeline\" extra to enable' ) # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # load model and processor self . model = VisionEncoderDecoderModel . from_pretrained ( self . path ) self . tokenizer = AutoTokenizer . from_pretrained ( self . path ) self . extractor = ViTFeatureExtractor . from_pretrained ( self . path ) # Move model to device self . model . to ( self . device )","title":"__init__()"},{"location":"pipeline/image/caption/#txtai.pipeline.image.caption.Caption.__call__","text":"Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Parameters: Name Type Description Default images image|list required Returns: Type Description list of captions Source code in txtai/pipeline/image/caption.py def __call__ ( self , images ): \"\"\" Builds captions for images. This method supports a single image or a list of images. If the input is an image, the return type is a string. If text is a list, a list of strings is returned Args: images: image|list Returns: list of captions \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Feature extraction pixels = self . extractor ( images = values , return_tensors = \"pt\" ) . pixel_values pixels = pixels . to ( self . device ) # Run model with torch . no_grad (): outputs = self . model . generate ( pixels , max_length = 16 , num_beams = 4 , return_dict_in_generate = True ) . sequences # Tokenize outputs into text results captions = self . tokenizer . batch_decode ( outputs , skip_special_tokens = True ) captions = [ caption . strip () for caption in captions ] # Return single element if single element passed in return captions [ 0 ] if not isinstance ( images , list ) else captions","title":"__call__()"},{"location":"pipeline/image/objects/","text":"Objects A objects pipeline reads a list of images and returns a list of detected objects. # Default configuration Objects () # Detect objects with an image classificaton model Objects ( classification = True , threshold = 0.95 ) __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ) special Source code in txtai/pipeline/image/objects.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ): if not PIL : raise ImportError ( 'Objects pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( \"image-classification\" if classification else \"object-detection\" , path , quantize , gpu , model ) self . classification = classification self . threshold = threshold __call__ ( self , images , flatten = False , workers = 0 ) special Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Parameters: Name Type Description Default images image|list required flatten flatten output to a list of objects False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (label, score) Source code in txtai/pipeline/image/objects.py def __call__ ( self , images , flatten = False , workers = 0 ): \"\"\" Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Args: images: image|list flatten: flatten output to a list of objects workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (label, score) \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Run pipeline results = ( self . pipeline ( values , num_workers = workers ) if self . classification else self . pipeline ( values , threshold = self . threshold , num_workers = workers ) ) # Build list of (id, score) outputs = [] for result in results : # Convert to (label, score) tuples result = [( x [ \"label\" ], x [ \"score\" ]) for x in result if x [ \"score\" ] > self . threshold ] # Sort by score descending result = sorted ( result , key = lambda x : x [ 1 ], reverse = True ) # Deduplicate labels unique = set () elements = [] for label , score in result : if label not in unique : elements . append ( label if flatten else ( label , score )) unique . add ( label ) outputs . append ( elements ) # Return single element if single element passed in return outputs [ 0 ] if not isinstance ( images , list ) else outputs","title":"Objects"},{"location":"pipeline/image/objects/#objects","text":"A objects pipeline reads a list of images and returns a list of detected objects. # Default configuration Objects () # Detect objects with an image classificaton model Objects ( classification = True , threshold = 0.95 )","title":"Objects"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__init__","text":"Source code in txtai/pipeline/image/objects.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , classification = False , threshold = 0.9 ): if not PIL : raise ImportError ( 'Objects pipeline is not available - install \"pipeline\" extra to enable' ) super () . __init__ ( \"image-classification\" if classification else \"object-detection\" , path , quantize , gpu , model ) self . classification = classification self . threshold = threshold","title":"__init__()"},{"location":"pipeline/image/objects/#txtai.pipeline.image.objects.Objects.__call__","text":"Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Parameters: Name Type Description Default images image|list required flatten flatten output to a list of objects False workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (label, score) Source code in txtai/pipeline/image/objects.py def __call__ ( self , images , flatten = False , workers = 0 ): \"\"\" Applies object detection/image classification models to images. Returns a list of (label, score). This method supports a single image or a list of images. If the input is an image, the return type is a 1D list of (label, score). If text is a list, a 2D list of (label, score) is returned with a row per image. Args: images: image|list flatten: flatten output to a list of objects workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (label, score) \"\"\" # Convert single element to list values = [ images ] if not isinstance ( images , list ) else images # Open images if file strings values = [ Image . open ( image ) if isinstance ( image , str ) else image for image in values ] # Run pipeline results = ( self . pipeline ( values , num_workers = workers ) if self . classification else self . pipeline ( values , threshold = self . threshold , num_workers = workers ) ) # Build list of (id, score) outputs = [] for result in results : # Convert to (label, score) tuples result = [( x [ \"label\" ], x [ \"score\" ]) for x in result if x [ \"score\" ] > self . threshold ] # Sort by score descending result = sorted ( result , key = lambda x : x [ 1 ], reverse = True ) # Deduplicate labels unique = set () elements = [] for label , score in result : if label not in unique : elements . append ( label if flatten else ( label , score )) unique . add ( label ) outputs . append ( elements ) # Return single element if single element passed in return outputs [ 0 ] if not isinstance ( images , list ) else outputs","title":"__call__()"},{"location":"pipeline/text/extractor/","text":"Extractor An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer ) __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ) special Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None minscore minimum score to include context match, defaults to None None mintokens minimum number of tokens to include context match, defaults to None None context topn context matches to include, defaults to 3 None Source code in txtai/pipeline/text/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class minscore: minimum score to include context match, defaults to None mintokens: minimum number of tokens to include context match, defaults to None context: topn context matches to include, defaults to 3 \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0 # Top N context matches to include for question-answering self . context = context if context else 3 __call__ ( self , queue , texts ) special Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/text/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: self . context ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"Extractor"},{"location":"pipeline/text/extractor/#extractor","text":"An Extractor pipeline is a combination of an embeddings query and an Extractive QA model. Filtering the context for a QA model helps maximize performance of the model. Extractor parameters are set as constructor arguments. Examples below. Extractor ( embeddings , path , quantize , gpu , model , tokenizer )","title":"Extractor"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__init__","text":"Builds a new extractor. Parameters: Name Type Description Default similarity similarity instance (embeddings or similarity instance) required path path to qa model required quantize True if model should be quantized before inference, False otherwise. False gpu if gpu inference should be used (only works if GPUs are available) True model optional existing pipeline model to wrap None tokenizer Tokenizer class None minscore minimum score to include context match, defaults to None None mintokens minimum number of tokens to include context match, defaults to None None context topn context matches to include, defaults to 3 None Source code in txtai/pipeline/text/extractor.py def __init__ ( self , similarity , path , quantize = False , gpu = True , model = None , tokenizer = None , minscore = None , mintokens = None , context = None ): \"\"\" Builds a new extractor. Args: similarity: similarity instance (embeddings or similarity instance) path: path to qa model quantize: True if model should be quantized before inference, False otherwise. gpu: if gpu inference should be used (only works if GPUs are available) model: optional existing pipeline model to wrap tokenizer: Tokenizer class minscore: minimum score to include context match, defaults to None mintokens: minimum number of tokens to include context match, defaults to None context: topn context matches to include, defaults to 3 \"\"\" # Similarity instance self . similarity = similarity # QA Pipeline self . pipeline = Questions ( path , quantize , gpu , model ) # Tokenizer class use default method if not set self . tokenizer = tokenizer if tokenizer else Tokenizer # Minimum score to include context match self . minscore = minscore if minscore is not None else 0.0 # Minimum number of tokens to include context match self . mintokens = mintokens if mintokens is not None else 0.0 # Top N context matches to include for question-answering self . context = context if context else 3","title":"__init__()"},{"location":"pipeline/text/extractor/#txtai.pipeline.text.extractor.Extractor.__call__","text":"Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Parameters: Name Type Description Default queue input queue (name, query, question, snippet) required texts list of text required Returns: Type Description list of (name, answer) Source code in txtai/pipeline/text/extractor.py def __call__ ( self , queue , texts ): \"\"\" Extracts answers to input questions. This method runs queries against a list of text, finds the top n best matches and uses that as the question context. A question-answering model is then run against the context for the input question, with the answer returned. Args: queue: input queue (name, query, question, snippet) texts: list of text Returns: list of (name, answer) \"\"\" # Execute embeddings query results = self . query ([ query for _ , query , _ , _ in queue ], texts ) # Build question-context pairs names , questions , contexts , topns , snippets = [], [], [], [], [] for x , ( name , _ , question , snippet ) in enumerate ( queue ): # Build context using top n best matching segments topn = sorted ( results [ x ], key = lambda y : y [ 2 ], reverse = True )[: self . context ] context = \" \" . join ([ text for _ , text , _ in sorted ( topn , key = lambda y : y [ 0 ])]) names . append ( name ) questions . append ( question ) contexts . append ( context ) topns . append ([ text for _ , text , _ in topn ]) snippets . append ( snippet ) # Run qa pipeline and return answers return self . answers ( names , questions , contexts , topns , snippets )","title":"__call__()"},{"location":"pipeline/text/labels/","text":"Labels A Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling). Labels parameters are set as constructor arguments. Examples below. # Default configuration Labels () # Custom model with zero shot classification Labels ( \"roberta-large-mnli\" ) # Model with fixed labels Labels ( dynamic = False ) __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ) special Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None False flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (id, score) or list of labels depending on flatten parameter Source code in txtai/pipeline/text/labels.py def __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (id, score) or list of labels depending on flatten parameter \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of outputs and return outputs = self . outputs ( results , labels , flatten ) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"Labels"},{"location":"pipeline/text/labels/#labels","text":"A Labels pipeline uses a text classification model to apply labels to input text. This pipeline can classify text using either a zero shot model (dynamic labeling) or a standard text classification model (fixed labeling). Labels parameters are set as constructor arguments. Examples below. # Default configuration Labels () # Custom model with zero shot classification Labels ( \"roberta-large-mnli\" ) # Model with fixed labels Labels ( dynamic = False )","title":"Labels"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__init__","text":"Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipeline/text/labels/#txtai.pipeline.text.labels.Labels.__call__","text":"Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default text text|list required labels list of labels None multilabel labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None False flatten flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description list of (id, score) or list of labels depending on flatten parameter Source code in txtai/pipeline/text/labels.py def __call__ ( self , text , labels = None , multilabel = False , flatten = None , workers = 0 ): \"\"\" Applies a text classifier to text. Returns a list of (id, score) sorted by highest score, where id is the index in labels. For zero shot classification, a list of labels is required. For text classification models, a list of labels is optional, otherwise all trained labels are returned. This method supports text as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: text: text|list labels: list of labels multilabel: labels are independent if True, scores are normalized to sum to 1 per text item if False, raw scores returned if None flatten: flatten output to a list of labels if present. Accepts a boolean or float value to only keep scores greater than that number. workers: number of concurrent workers to use for processing data, defaults to None Returns: list of (id, score) or list of labels depending on flatten parameter \"\"\" if self . dynamic : # Run zero shot classification pipeline results = self . pipeline ( text , labels , multi_label = multilabel , truncation = True , num_workers = workers ) else : # Set classification function based on inputs function = \"none\" if multilabel is None else \"sigmoid\" if multilabel or len ( self . labels ()) == 1 else \"softmax\" # Run text classification pipeline results = self . pipeline ( text , return_all_scores = True , function_to_apply = function , num_workers = workers ) # Convert results to a list if necessary if not isinstance ( results , list ): results = [ results ] # Build list of outputs and return outputs = self . outputs ( results , labels , flatten ) return outputs [ 0 ] if isinstance ( text , str ) else outputs","title":"__call__()"},{"location":"pipeline/text/similarity/","text":"Similarity A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ) special Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic __call__ ( self , query , texts , multilabel = True ) special Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/text/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"Similarity"},{"location":"pipeline/text/similarity/#similarity","text":"A Similarity pipeline is also a zero shot classifier model where the labels are the queries. The results are transposed to get scores per query/label vs scores per input text. Similarity parameters are set as constructor arguments. Examples below. Similarity () Similarity ( \"roberta-large-mnli\" )","title":"Similarity"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.labels.Similarity.__init__","text":"Source code in txtai/pipeline/text/labels.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None , dynamic = True ): super () . __init__ ( \"zero-shot-classification\" if dynamic else \"text-classification\" , path , quantize , gpu , model ) # Set if labels are dynamic (zero shot) or fixed (standard text classification) self . dynamic = dynamic","title":"__init__()"},{"location":"pipeline/text/similarity/#txtai.pipeline.text.similarity.Similarity.__call__","text":"Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Parameters: Name Type Description Default query query text|list required texts list of text required Returns: Type Description list of (id, score) Source code in txtai/pipeline/text/similarity.py def __call__ ( self , query , texts , multilabel = True ): \"\"\" Computes the similarity between query and list of text. Returns a list of (id, score) sorted by highest score, where id is the index in texts. This method supports query as a string or a list. If the input is a string, the return type is a 1D list of (id, score). If text is a list, a 2D list of (id, score) is returned with a row per string. Args: query: query text|list texts: list of text Returns: list of (id, score) \"\"\" # Call Labels pipeline for texts using input query as the candidate label scores = super () . __call__ ( texts , [ query ] if isinstance ( query , str ) else query , multilabel ) # Sort on query index id scores = [[ score for _ , score in sorted ( row )] for row in scores ] # Transpose axes to get a list of text scores for each query scores = np . array ( scores ) . T . tolist () # Build list of (id, score) per query sorted by highest score scores = [ sorted ( enumerate ( row ), key = lambda x : x [ 1 ], reverse = True ) for row in scores ] return scores [ 0 ] if isinstance ( query , str ) else scores","title":"__call__()"},{"location":"pipeline/text/summary/","text":"Summary A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" ) __init__ ( self , path = None , quantize = False , gpu = True , model = None ) special Source code in txtai/pipeline/text/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model ) __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ) special Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description summary text Source code in txtai/pipeline/text/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of concurrent workers to use for processing data, defaults to None Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"Summary"},{"location":"pipeline/text/summary/#summary","text":"A Summary pipeline summarizes text. Summary parameters are set as constructor arguments. Examples below. Summary () Summary ( \"sshleifer/distilbart-cnn-12-3\" )","title":"Summary"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__init__","text":"Source code in txtai/pipeline/text/summary.py def __init__ ( self , path = None , quantize = False , gpu = True , model = None ): super () . __init__ ( \"summarization\" , path , quantize , gpu , model )","title":"__init__()"},{"location":"pipeline/text/summary/#txtai.pipeline.text.summary.Summary.__call__","text":"Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Parameters: Name Type Description Default text text|list required minlength minimum length for summary None maxlength maximum length for summary None workers number of concurrent workers to use for processing data, defaults to None 0 Returns: Type Description summary text Source code in txtai/pipeline/text/summary.py def __call__ ( self , text , minlength = None , maxlength = None , workers = 0 ): \"\"\" Runs a summarization model against a block of text. This method supports text as a string or a list. If the input is a string, the return type is text. If text is a list, a list of text is returned with a row per block of text. Args: text: text|list minlength: minimum length for summary maxlength: maximum length for summary workers: number of concurrent workers to use for processing data, defaults to None Returns: summary text \"\"\" # Validate text length greater than max length check = maxlength if maxlength else self . pipeline . model . config . max_length # Skip text shorter than max length texts = text if isinstance ( text , list ) else [ text ] params = [( x , text if len ( text ) >= check else None ) for x , text in enumerate ( texts )] kwargs = { \"truncation\" : True } if minlength : kwargs [ \"min_length\" ] = minlength if maxlength : kwargs [ \"max_length\" ] = maxlength inputs = [ text for _ , text in params if text ] if inputs : # Run summarization pipeline results = self . pipeline ( inputs , num_workers = workers , ** kwargs ) # Pull out summary text results = iter ([ self . clean ( x [ \"summary_text\" ]) for x in results ]) results = [ next ( results ) if text else texts [ x ] for x , text in params ] else : # Return original results = texts return results [ 0 ] if isinstance ( text , str ) else results","title":"__call__()"},{"location":"pipeline/text/translation/","text":"Translation A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation () __init__ ( self , path = 'facebook/m2m100_418M' , quantize = False , gpu = True , batch = 64 , langdetect = 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' ) special Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/text/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available () __call__ ( self , texts , target = 'en' , source = None ) special Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/text/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"Translation"},{"location":"pipeline/text/translation/#translation","text":"A Translation pipeline translates text between languages Translation parameters are set as constructor arguments. Examples below. Translation ()","title":"Translation"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__init__","text":"Constructs a new language translation pipeline. Parameters: Name Type Description Default path optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided 'facebook/m2m100_418M' quantize if model should be quantized, defaults to False False gpu True/False if GPU should be enabled, also supports a GPU device id True batch batch size used to incrementally process content 64 langdetect path to language detection model, uses a default path if not provided 'https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz' Source code in txtai/pipeline/text/translation.py def __init__ ( self , path = \"facebook/m2m100_418M\" , quantize = False , gpu = True , batch = 64 , langdetect = DEFAULT_LANG_DETECT ): \"\"\" Constructs a new language translation pipeline. Args: path: optional path to model, accepts Hugging Face model hub id or local path, uses default model for task if not provided quantize: if model should be quantized, defaults to False gpu: True/False if GPU should be enabled, also supports a GPU device id batch: batch size used to incrementally process content langdetect: path to language detection model, uses a default path if not provided \"\"\" # Call parent constructor super () . __init__ ( path , quantize , gpu , batch ) # Language detection self . detector = None self . langdetect = langdetect # Language models self . models = {} self . ids = self . available ()","title":"__init__()"},{"location":"pipeline/text/translation/#txtai.pipeline.text.translation.Translation.__call__","text":"Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Parameters: Name Type Description Default texts text|list required target target language code, defaults to \"en\" 'en' source source language code, detects language if not provided None Returns: Type Description list of translated text Source code in txtai/pipeline/text/translation.py def __call__ ( self , texts , target = \"en\" , source = None ): \"\"\" Translates text from source language into target language. This method supports texts as a string or a list. If the input is a string, the return type is string. If text is a list, the return type is a list. Args: texts: text|list target: target language code, defaults to \"en\" source: source language code, detects language if not provided Returns: list of translated text \"\"\" values = [ texts ] if not isinstance ( texts , list ) else texts # Detect source languages languages = self . detect ( values ) if not source else [ source ] * len ( values ) unique = set ( languages ) # Build list of (index, language, text) values = [( x , lang , values [ x ]) for x , lang in enumerate ( languages )] results = {} for language in unique : # Get all text values for language inputs = [( x , text ) for x , lang , text in values if lang == language ] # Translate text in batches outputs = [] for chunk in self . batch ([ text for _ , text in inputs ], self . batchsize ): outputs . extend ( self . translate ( chunk , language , target )) # Store output value for y , ( x , _ ) in enumerate ( inputs ): results [ x ] = outputs [ y ] # Return results in same order as input results = [ results [ x ] for x in sorted ( results )] return results [ 0 ] if isinstance ( texts , str ) else results","title":"__call__()"},{"location":"pipeline/train/hfonnx/","text":"HFOnnx Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation). Example on how to use the pipeline below. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" ) __init__ ( / , self , * args , ** kwargs ) special __call__ ( self , path , task = 'default' , output = None , quantize = False , opset = 12 ) special Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"HF ONNX"},{"location":"pipeline/train/hfonnx/#hfonnx","text":"Exports a Hugging Face Transformer model to ONNX. Currently, this works best with classification/pooling/qa models. Work is ongoing for sequence to sequence models (summarization, transcription, translation). Example on how to use the pipeline below. from txtai.pipeline import HFOnnx , Labels # Model path path = \"distilbert-base-uncased-finetuned-sst-2-english\" # Export model to ONNX onnx = HFOnnx () model = onnx ( path , \"text-classification\" , \"model.onnx\" , True ) # Run inference and validate labels = Labels (( model , path ), dynamic = False ) labels ( \"I am happy\" )","title":"HFOnnx"},{"location":"pipeline/train/hfonnx/#txtai.pipeline.train.hfonnx.HFOnnx.__init__","text":"","title":"__init__()"},{"location":"pipeline/train/hfonnx/#txtai.pipeline.train.hfonnx.HFOnnx.__call__","text":"Exports a Hugging Face Transformer model to ONNX. Parameters: Name Type Description Default path path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required task optional model task or category, determines the model type and outputs, defaults to export hidden state 'default' output optional output model path, defaults to return byte array if None None quantize if model should be quantized (requires onnx to be installed), defaults to False False opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/hfonnx.py def __call__ ( self , path , task = \"default\" , output = None , quantize = False , opset = 12 ): \"\"\" Exports a Hugging Face Transformer model to ONNX. Args: path: path to model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple task: optional model task or category, determines the model type and outputs, defaults to export hidden state output: optional output model path, defaults to return byte array if None quantize: if model should be quantized (requires onnx to be installed), defaults to False opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" inputs , outputs , model = self . parameters ( task ) if isinstance ( path , ( list , tuple )): model , tokenizer = path model = model . cpu () else : model = model ( path ) tokenizer = AutoTokenizer . from_pretrained ( path ) # Generate dummy inputs dummy = dict ( tokenizer ([ \"test inputs\" ], return_tensors = \"pt\" )) # Default to BytesIO if no output file provided output = output if output else BytesIO () # Export model to ONNX export ( model , ( dummy ,), output , opset_version = opset , do_constant_folding = True , input_names = list ( inputs . keys ()), output_names = list ( outputs . keys ()), dynamic_axes = dict ( chain ( inputs . items (), outputs . items ())), ) # Quantize model if quantize : if not ONNX_RUNTIME : raise ImportError ( 'onnxruntime is not available - install \"pipeline\" extra to enable' ) output = self . quantization ( output ) if isinstance ( output , BytesIO ): # Reset stream and return bytes output . seek ( 0 ) output = output . read () return output","title":"__call__()"},{"location":"pipeline/train/mlonnx/","text":"MLOnnx Exports a traditional machine learning model (i.e. scikit-learn) to ONNX. __init__ ( self ) special Creates a new MLOnnx pipeline. Source code in txtai/pipeline/train/mlonnx.py def __init__ ( self ): \"\"\" Creates a new MLOnnx pipeline. \"\"\" if not ONNX_MLTOOLS : raise ImportError ( 'MLOnnx pipeline is not available - install \"pipeline\" extra to enable' ) __call__ ( self , model , task = 'default' , output = None , opset = 12 ) special Exports a machine learning model to ONNX using ONNXMLTools. Parameters: Name Type Description Default model model to export required task optional model task or category 'default' output optional output model path, defaults to return byte array if None None opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/mlonnx.py def __call__ ( self , model , task = \"default\" , output = None , opset = 12 ): \"\"\" Exports a machine learning model to ONNX using ONNXMLTools. Args: model: model to export task: optional model task or category output: optional output model path, defaults to return byte array if None opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" # Convert scikit-learn model to ONNX model = convert_sklearn ( model , task , initial_types = [( \"input_ids\" , StringTensorType ([ None , None ]))], target_opset = opset ) # Prune model graph down to only output probabilities model = select_model_inputs_outputs ( model , outputs = \"probabilities\" ) # pylint: disable=E1101 # Rename output to logits for consistency with other models model . graph . output [ 0 ] . name = \"logits\" model . graph . node [ 0 ] . output [ 0 ] = \"logits\" # Save model to specified output path or return bytes model = save_onnx_model ( model , output ) return output if output else model","title":"ML ONNX"},{"location":"pipeline/train/mlonnx/#mlonnx","text":"Exports a traditional machine learning model (i.e. scikit-learn) to ONNX.","title":"MLOnnx"},{"location":"pipeline/train/mlonnx/#txtai.pipeline.train.mlonnx.MLOnnx.__init__","text":"Creates a new MLOnnx pipeline. Source code in txtai/pipeline/train/mlonnx.py def __init__ ( self ): \"\"\" Creates a new MLOnnx pipeline. \"\"\" if not ONNX_MLTOOLS : raise ImportError ( 'MLOnnx pipeline is not available - install \"pipeline\" extra to enable' )","title":"__init__()"},{"location":"pipeline/train/mlonnx/#txtai.pipeline.train.mlonnx.MLOnnx.__call__","text":"Exports a machine learning model to ONNX using ONNXMLTools. Parameters: Name Type Description Default model model to export required task optional model task or category 'default' output optional output model path, defaults to return byte array if None None opset onnx opset, defaults to 12 12 Returns: Type Description path to model output or model as bytes depending on output parameter Source code in txtai/pipeline/train/mlonnx.py def __call__ ( self , model , task = \"default\" , output = None , opset = 12 ): \"\"\" Exports a machine learning model to ONNX using ONNXMLTools. Args: model: model to export task: optional model task or category output: optional output model path, defaults to return byte array if None opset: onnx opset, defaults to 12 Returns: path to model output or model as bytes depending on output parameter \"\"\" # Convert scikit-learn model to ONNX model = convert_sklearn ( model , task , initial_types = [( \"input_ids\" , StringTensorType ([ None , None ]))], target_opset = opset ) # Prune model graph down to only output probabilities model = select_model_inputs_outputs ( model , outputs = \"probabilities\" ) # pylint: disable=E1101 # Rename output to logits for consistency with other models model . graph . output [ 0 ] . name = \"logits\" model . graph . node [ 0 ] . output [ 0 ] = \"logits\" # Save model to specified output path or return bytes model = save_onnx_model ( model , output ) return output if output else model","title":"__call__()"},{"location":"pipeline/train/trainer/","text":"HFTrainer Trains a new Hugging Face Transformer model using the Trainer framework. Examples on how to use the trainer below. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call. __init__ ( / , self , * args , ** kwargs ) special __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = 'text-classification' , ** args ) special Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/train/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"Trainer"},{"location":"pipeline/train/trainer/#hftrainer","text":"Trains a new Hugging Face Transformer model using the Trainer framework. Examples on how to use the trainer below. import pandas as pd from datasets import load_dataset from txtai.pipeline import HFTrainer trainer = HFTrainer () # Pandas DataFrame df = pd . read_csv ( \"training.csv\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , df ) # Hugging Face dataset ds = load_dataset ( \"glue\" , \"sst2\" ) model , tokenizer = trainer ( \"bert-base-uncased\" , ds [ \"train\" ], columns = ( \"sentence\" , \"label\" )) # List of dicts dt = [{ \"text\" : \"sentence 1\" , \"label\" : 0 }, { \"text\" : \"sentence 2\" , \"label\" : 1 }]] model , tokenizer = trainer ( \"bert-base-uncased\" , dt ) # Support additional TrainingArguments model , tokenizer = trainer ( \"bert-base-uncased\" , dt , learning_rate = 3e-5 , num_train_epochs = 5 ) All TrainingArguments are supported as function arguments to the trainer call.","title":"HFTrainer"},{"location":"pipeline/train/trainer/#txtai.pipeline.train.hftrainer.HFTrainer.__init__","text":"","title":"__init__()"},{"location":"pipeline/train/trainer/#txtai.pipeline.train.hftrainer.HFTrainer.__call__","text":"Builds a new model using arguments. Parameters: Name Type Description Default base path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple required train training data required validation validation data None columns tuple of columns to use for text/label, defaults to (text, None, label) None maxlength maximum sequence length, defaults to tokenizer.model_max_length None stride chunk size for splitting data for QA tasks 128 task optional model task or category, determines the model type, defaults to \"text-classification\" 'text-classification' args training arguments {} Returns: Type Description (model, tokenizer) Source code in txtai/pipeline/train/hftrainer.py def __call__ ( self , base , train , validation = None , columns = None , maxlength = None , stride = 128 , task = \"text-classification\" , ** args ): \"\"\" Builds a new model using arguments. Args: base: path to base model, accepts Hugging Face model hub id, local path or (model, tokenizer) tuple train: training data validation: validation data columns: tuple of columns to use for text/label, defaults to (text, None, label) maxlength: maximum sequence length, defaults to tokenizer.model_max_length stride: chunk size for splitting data for QA tasks task: optional model task or category, determines the model type, defaults to \"text-classification\" args: training arguments Returns: (model, tokenizer) \"\"\" # Parse TrainingArguments args = self . parse ( args ) # Set seed for model reproducibility set_seed ( args . seed ) # Load model configuration, tokenizer and max sequence length config , tokenizer , maxlength = self . load ( base , maxlength ) # List of labels (only for classification models) labels = None # Prepare datasets if task == \"question-answering\" : process = Questions ( tokenizer , columns , maxlength , stride ) else : process = Labels ( tokenizer , columns , maxlength ) labels = process . labels ( train ) # Tokenize training and validation data train , validation = process ( train , validation ) # Create model to train model = self . model ( task , base , config , labels ) # Build trainer trainer = Trainer ( model = model , tokenizer = tokenizer , args = args , train_dataset = train , eval_dataset = validation if validation else None ) # Run training trainer . train () # Run evaluation if validation : trainer . evaluate () # Save model outputs if args . should_save : trainer . save_model () trainer . save_state () # Put model in eval mode to disable weight updates and return (model, tokenizer) return ( model . eval (), tokenizer )","title":"__call__()"},{"location":"workflow/file/","text":"File Task Task that processes file urls. workflow = Workflow ([ FileTask ()]) workflow ([ \"/path/to/file\" , \"file:///path/to/file\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"File"},{"location":"workflow/file/#file-task","text":"Task that processes file urls. workflow = Workflow ([ FileTask ()]) workflow ([ \"/path/to/file\" , \"file:///path/to/file\" ])","title":"File Task"},{"location":"workflow/file/#txtai.workflow.task.base.FileTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/image/","text":"Image Task Task that processes image urls. workflow = Workflow ([ ImageTask ()]) workflow ([ \"image.jpg\" , \"image.gif\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Image"},{"location":"workflow/image/#image-task","text":"Task that processes image urls. workflow = Workflow ([ ImageTask ()]) workflow ([ \"image.jpg\" , \"image.gif\" ])","title":"Image Task"},{"location":"workflow/image/#txtai.workflow.task.base.ImageTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/overview/","text":"Workflow Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Transformers models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. An example of the most basic workflow: workflow = Workflow ([ Task ( lambda x : [ y * 2 for y in x ])]) list ( workflow ([ 1 , 2 , 3 ])) This example simply multiplies each input value and returns a outputs via a generator. A more complex example is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass Workflows can be defined using Python as described here but they can also be created as YAML configuration and run as shown below. # Create and run the workflow app = API ( \"workflow.yml\" ) data = list ( app . workflow ( \"workflow\" , [ \"input text\" ])) Read more here on creating workflow YAML . Workflow Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements. __init__ ( self , tasks , batch = 100 , workers = None ) special Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 workers number of concurrent workers None Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 , workers = None ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 workers: number of concurrent workers \"\"\" self . tasks = tasks self . batch = batch self . workers = workers # Set default number of executor workers to max number of actions in a task self . workers = max ( len ( task . action ) for task in self . tasks ) if not self . workers else self . workers __call__ ( self , elements ) special Executes a workflow for input elements. Parameters: Name Type Description Default elements iterable data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: iterable data elements Returns: transformed data elements \"\"\" # Create execute instance for this run with Execute ( self . workers ) as executor : # Run task initializers self . initialize () # Process elements in batches for batch in self . chunk ( elements ): yield from self . process ( batch , executor ) # Run task finalizers self . finalize ()","title":"Overview"},{"location":"workflow/overview/#workflow","text":"Workflows are a simple yet powerful construct that takes a callable and returns elements. Workflows operate well with pipelines but can work with any callable object. Workflows are streaming by nature and work on data in batches, allowing large volumes of data to be processed efficiently. Given that pipelines are callable objects, workflows enable efficient processing of pipeline data. Transformers models typically work with smaller batches of data, workflows are well suited to feed a series of transformers pipelines. An example of the most basic workflow: workflow = Workflow ([ Task ( lambda x : [ y * 2 for y in x ])]) list ( workflow ([ 1 , 2 , 3 ])) This example simply multiplies each input value and returns a outputs via a generator. A more complex example is shown below. This workflow will work with both documents and audio files. Documents will have text extracted and summarized. Audio files will be transcribed. Both results will be joined, translated into French and loaded into an Embeddings index. # file:// prefixes are required to signal to the workflow this is a file and not a text string files = [ \"file://txtai/article.pdf\" , \"file://txtai/US_tops_5_million.wav\" , \"file://txtai/Canadas_last_fully.wav\" , \"file://txtai/Beijing_mobilises.wav\" , \"file://txtai/The_National_Park.wav\" , \"file://txtai/Maine_man_wins_1_mil.wav\" , \"file://txtai/Make_huge_profits.wav\" ] data = [( x , element , None ) for x , element in enumerate ( files )] # Workflow that extracts text and builds a summary articles = Workflow ([ FileTask ( textractor ), Task ( lambda x : summary ([ y [: 1024 ] for y in x ])) ]) # Define workflow tasks. Workflows can also be tasks! tasks = [ WorkflowTask ( articles , r \".\\.pdf$\" ), FileTask ( transcribe , r \"\\.wav$\" ), Task ( lambda x : translate ( x , \"fr\" )), Task ( index , unpack = False ) ] # Workflow that translates text to French workflow = Workflow ( tasks ) for _ in workflow ( data ): pass Workflows can be defined using Python as described here but they can also be created as YAML configuration and run as shown below. # Create and run the workflow app = API ( \"workflow.yml\" ) data = list ( app . workflow ( \"workflow\" , [ \"input text\" ])) Read more here on creating workflow YAML .","title":"Workflow"},{"location":"workflow/overview/#workflow_1","text":"Workflows are callable objects. Workflows take an input of iterable data elements and output iterable data elements.","title":"Workflow"},{"location":"workflow/overview/#txtai.workflow.base.Workflow.__init__","text":"Creates a new workflow. Workflows are lists of tasks to execute. Parameters: Name Type Description Default tasks list of workflow tasks required batch how many items to process at a time, defaults to 100 100 workers number of concurrent workers None Source code in txtai/workflow/base.py def __init__ ( self , tasks , batch = 100 , workers = None ): \"\"\" Creates a new workflow. Workflows are lists of tasks to execute. Args: tasks: list of workflow tasks batch: how many items to process at a time, defaults to 100 workers: number of concurrent workers \"\"\" self . tasks = tasks self . batch = batch self . workers = workers # Set default number of executor workers to max number of actions in a task self . workers = max ( len ( task . action ) for task in self . tasks ) if not self . workers else self . workers","title":"__init__()"},{"location":"workflow/overview/#txtai.workflow.base.Workflow.__call__","text":"Executes a workflow for input elements. Parameters: Name Type Description Default elements iterable data elements required Returns: Type Description transformed data elements Source code in txtai/workflow/base.py def __call__ ( self , elements ): \"\"\" Executes a workflow for input elements. Args: elements: iterable data elements Returns: transformed data elements \"\"\" # Create execute instance for this run with Execute ( self . workers ) as executor : # Run task initializers self . initialize () # Process elements in batches for batch in self . chunk ( elements ): yield from self . process ( batch , executor ) # Run task finalizers self . finalize ()","title":"__call__()"},{"location":"workflow/retrieve/","text":"Retrieve Task Task that connects to a url and downloads the content locally. workflow = Workflow ([ RetrieveTask ( directory = \"/tmp\" )]) workflow ([ \"https://file.to.download\" , \"/local/file/to/copy\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) register ( self , directory = None ) Adds retrieve parameters to task. Parameters: Name Type Description Default directory local directory used to store retrieved files None Source code in txtai/workflow/task/retrieve.py def register ( self , directory = None ): \"\"\" Adds retrieve parameters to task. Args: directory: local directory used to store retrieved files \"\"\" # pylint: disable=W0201 # Create default temporary directory if not specified if not directory : # Save tempdir to prevent content from being deleted until this task is out of scope # pylint: disable=R1732 self . tempdir = tempfile . TemporaryDirectory () directory = self . tempdir . name # Create output directory if necessary if not os . path . exists ( directory ): os . makedirs ( directory ) self . directory = directory","title":"Retrieve"},{"location":"workflow/retrieve/#retrieve-task","text":"Task that connects to a url and downloads the content locally. workflow = Workflow ([ RetrieveTask ( directory = \"/tmp\" )]) workflow ([ \"https://file.to.download\" , \"/local/file/to/copy\" ])","title":"Retrieve Task"},{"location":"workflow/retrieve/#txtai.workflow.task.base.RetrieveTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/retrieve/#txtai.workflow.task.retrieve.RetrieveTask.register","text":"Adds retrieve parameters to task. Parameters: Name Type Description Default directory local directory used to store retrieved files None Source code in txtai/workflow/task/retrieve.py def register ( self , directory = None ): \"\"\" Adds retrieve parameters to task. Args: directory: local directory used to store retrieved files \"\"\" # pylint: disable=W0201 # Create default temporary directory if not specified if not directory : # Save tempdir to prevent content from being deleted until this task is out of scope # pylint: disable=R1732 self . tempdir = tempfile . TemporaryDirectory () directory = self . tempdir . name # Create output directory if necessary if not os . path . exists ( directory ): os . makedirs ( directory ) self . directory = directory","title":"register()"},{"location":"workflow/service/","text":"Service Task Task that runs content against a http service. workflow = Workflow ([ ServiceTask ( url = \"https://service.url/action)]) workflow ([ \"parameter\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) register ( self , url = None , method = None , params = None , batch = True , extract = None ) Adds service parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default url url to connect to None method http method, GET or POST None params default query parameters None batch if True, all elements are passed in a single batch request, otherwise a service call is executed per element True extract list of sections to extract from response None Source code in txtai/workflow/task/service.py def register ( self , url = None , method = None , params = None , batch = True , extract = None ): \"\"\" Adds service parameters to task. Checks if required dependencies are installed. Args: url: url to connect to method: http method, GET or POST params: default query parameters batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element extract: list of sections to extract from response \"\"\" if not XML_TO_DICT : raise ImportError ( 'ServiceTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 # Save URL, method and parameter defaults self . url = url self . method = method self . params = params # If True, all elements are passed in a single batch request, otherwise a service call is executed per element self . batch = batch # Save sections to extract. Supports both a single string and a hierarchical list of sections. self . extract = extract if self . extract : self . extract = [ self . extract ] if isinstance ( self . extract , str ) else self . extract","title":"Service"},{"location":"workflow/service/#service-task","text":"Task that runs content against a http service. workflow = Workflow ([ ServiceTask ( url = \"https://service.url/action)]) workflow ([ \"parameter\" ])","title":"Service Task"},{"location":"workflow/service/#txtai.workflow.task.base.ServiceTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/service/#txtai.workflow.task.service.ServiceTask.register","text":"Adds service parameters to task. Checks if required dependencies are installed. Parameters: Name Type Description Default url url to connect to None method http method, GET or POST None params default query parameters None batch if True, all elements are passed in a single batch request, otherwise a service call is executed per element True extract list of sections to extract from response None Source code in txtai/workflow/task/service.py def register ( self , url = None , method = None , params = None , batch = True , extract = None ): \"\"\" Adds service parameters to task. Checks if required dependencies are installed. Args: url: url to connect to method: http method, GET or POST params: default query parameters batch: if True, all elements are passed in a single batch request, otherwise a service call is executed per element extract: list of sections to extract from response \"\"\" if not XML_TO_DICT : raise ImportError ( 'ServiceTask is not available - install \"workflow\" extra to enable' ) # pylint: disable=W0201 # Save URL, method and parameter defaults self . url = url self . method = method self . params = params # If True, all elements are passed in a single batch request, otherwise a service call is executed per element self . batch = batch # Save sections to extract. Supports both a single string and a hierarchical list of sections. self . extract = extract if self . extract : self . extract = [ self . extract ] if isinstance ( self . extract , str ) else self . extract","title":"register()"},{"location":"workflow/storage/","text":"Storage Task Task that expands a local directory or cloud storage bucket into a list of URLs to process. workflow = Workflow ([ StorageTask ()]) workflow ([ \"s3://path/to/bucket\" , \"local://local/directory\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Storage"},{"location":"workflow/storage/#storage-task","text":"Task that expands a local directory or cloud storage bucket into a list of URLs to process. workflow = Workflow ([ StorageTask ()]) workflow ([ \"s3://path/to/bucket\" , \"local://local/directory\" ])","title":"Storage Task"},{"location":"workflow/storage/#txtai.workflow.task.base.StorageTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/","text":"Task Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. A simple task is shown below. Task ( lambda x : [ y * 2 for y in x ]) The task above executes the function above for all input elements. Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element. summary = Summary () Task ( summary ) Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing. summary = Summary () task = Task ( summary ) task ([ \"Very long text here\" ]) workflow = Workflow ([ task ]) list ( workflow ([ \"Very long text here\" ])) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Parameters: Name Type Description Default action action(s) to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True column column index to select if element is a tuple, defaults to all None merge merge mode for joining multi-action outputs, defaults to hstack 'hstack' initialize action to execute before processing None finalize action to execute after processing None concurrency sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency None kwargs additional keyword arguments {} Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" ) Multi-action task concurrency The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks. In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below. multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions. multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions. More information on multiprocessing can be found in the Python documentation . Multi-action task merges Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways. hstack ( self , outputs ) Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as tuples (column-wise) Source code in txtai/workflow/task/base.py def hstack ( self , outputs ): \"\"\" Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as tuples (column-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . stack ( outputs , axis = 1 ) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . stack ( outputs , axis = 1 ) return list ( zip ( * outputs )) vstack ( self , outputs ) Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as one to many transforms (row-wise) Source code in txtai/workflow/task/base.py def vstack ( self , outputs ): \"\"\" Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as one to many transforms (row-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . concatenate ( np . stack ( outputs , axis = 1 )) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . cat ( tuple ( torch . stack ( outputs , axis = 1 ))) # Flatten into lists of outputs per input row. Wrap as one to many transformation. merge = [] for x in zip ( * outputs ): combine = [] for y in x : if isinstance ( y , list ): combine . extend ( y ) else : combine . append ( y ) merge . append ( OneToMany ( combine )) return merge concat ( self , outputs ) Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of concat outputs Source code in txtai/workflow/task/base.py def concat ( self , outputs ): \"\"\" Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Args: outputs: task outputs Returns: list of concat outputs \"\"\" return [ \". \" . join ([ str ( y ) for y in x if y ]) for x in self . hstack ( outputs )] Extract task output columns With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element. A simple example: workflow = Workflow ([ Task ( lambda x : [ y * 3 for y in x ], unpack = False , column = 0 )]) list ( workflow ([( 2 , 8 )])) For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. workflow = Workflow ([ Task ([ lambda x : [ y * 3 for y in x ], lambda x : [ y - 1 for y in x ]], unpack = False , column = { 0 : 0 , 1 : 1 })]) list ( workflow ([( 2 , 8 )])) The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!","title":"Task"},{"location":"workflow/task/#task","text":"Workflows execute tasks. Tasks are callable objects with a number of parameters to control the processing of data at a given step. A simple task is shown below. Task ( lambda x : [ y * 2 for y in x ]) The task above executes the function above for all input elements. Tasks work well with pipelines, since pipelines are callable objects. The example below will summarize each input element. summary = Summary () Task ( summary ) Tasks can operate independently but work best with workflows, as workflows add large-scale stream processing. summary = Summary () task = Task ( summary ) task ([ \"Very long text here\" ]) workflow = Workflow ([ task ]) list ( workflow ([ \"Very long text here\" ]))","title":"Task"},{"location":"workflow/task/#txtai.workflow.task.base.Task.__init__","text":"Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Parameters: Name Type Description Default action action(s) to execute on each data element None select filter(s) used to select data to process None unpack if data elements should be unpacked or unwrapped from (id, data, tag) tuples True column column index to select if element is a tuple, defaults to all None merge merge mode for joining multi-action outputs, defaults to hstack 'hstack' initialize action to execute before processing None finalize action to execute after processing None concurrency sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency None kwargs additional keyword arguments {} Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/task/#multi-action-task-concurrency","text":"The default processing mode is to run actions sequentially. Multiprocessing support is already built in at a number of levels. Any of the GPU models will maximize GPU utilization for example and even in CPU mode, concurrency is utilized. But there are still use cases for task action concurrency. For example, if the system has multiple GPUs, the task runs external sequential code, or the task has a large number of I/O tasks. In addition to sequential processing, multi-action tasks can run either multithreaded or with multiple processes. The advantages of each approach are discussed below. multithreading - no overhead of creating separate processes or pickling data. But Python can only execute a single thread due the GIL, so this approach won't help with CPU bound actions. This method works well with I/O bound actions and GPU actions. multiprocessing - separate subprocesses are created and data is exchanged via pickling. This method can fully utilize all CPU cores since each process runs independently. This method works well with CPU bound actions. More information on multiprocessing can be found in the Python documentation .","title":"Multi-action task concurrency"},{"location":"workflow/task/#multi-action-task-merges","text":"Multi-action tasks will generate parallel outputs for the input data. The task output can be merged together in a couple different ways.","title":"Multi-action task merges"},{"location":"workflow/task/#txtai.workflow.task.base.Task.hstack","text":"Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as tuples (column-wise) Source code in txtai/workflow/task/base.py def hstack ( self , outputs ): \"\"\" Merges outputs column-wise. Returns a list of tuples which will be interpreted as a one to one transformation. Column-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Column Merge => [(a1, a2), (b1, b2), (c1, c2)] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as tuples (column-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . stack ( outputs , axis = 1 ) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . stack ( outputs , axis = 1 ) return list ( zip ( * outputs ))","title":"hstack()"},{"location":"workflow/task/#txtai.workflow.task.base.Task.vstack","text":"Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of aggregated/zipped outputs as one to many transforms (row-wise) Source code in txtai/workflow/task/base.py def vstack ( self , outputs ): \"\"\" Merges outputs row-wise. Returns a list of lists which will be interpreted as a one to many transformation. Row-wise merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Row Merge => [[a1, a2], [b1, b2], [c1, c2]] = [a1, a2, b1, b2, c1, c2] Args: outputs: task outputs Returns: list of aggregated/zipped outputs as one to many transforms (row-wise) \"\"\" # If all outputs are numpy arrays, use native method if all ( isinstance ( output , np . ndarray ) for output in outputs ): return np . concatenate ( np . stack ( outputs , axis = 1 )) # If all outputs are torch tensors, use native method # pylint: disable=E1101 if all ( torch . is_tensor ( output ) for output in outputs ): return torch . cat ( tuple ( torch . stack ( outputs , axis = 1 ))) # Flatten into lists of outputs per input row. Wrap as one to many transformation. merge = [] for x in zip ( * outputs ): combine = [] for y in x : if isinstance ( y , list ): combine . extend ( y ) else : combine . append ( y ) merge . append ( OneToMany ( combine )) return merge","title":"vstack()"},{"location":"workflow/task/#txtai.workflow.task.base.Task.concat","text":"Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Parameters: Name Type Description Default outputs task outputs required Returns: Type Description list of concat outputs Source code in txtai/workflow/task/base.py def concat ( self , outputs ): \"\"\" Merges outputs column-wise and concats values together into a string. Returns a list of strings. Concat merge example (2 actions) Inputs: [a, b, c] Outputs => [[a1, b1, c1], [a2, b2, c2]] Concat Merge => [(a1, a2), (b1, b2), (c1, c2)] => [\"a1. a2\", \"b1. b2\", \"c1. c2\"] Args: outputs: task outputs Returns: list of concat outputs \"\"\" return [ \". \" . join ([ str ( y ) for y in x if y ]) for x in self . hstack ( outputs )]","title":"concat()"},{"location":"workflow/task/#extract-task-output-columns","text":"With column-wise merging, each output row will be a tuple of output values for each task action. This can be fed as input to a downstream task and that task can have separate tasks work with each element. A simple example: workflow = Workflow ([ Task ( lambda x : [ y * 3 for y in x ], unpack = False , column = 0 )]) list ( workflow ([( 2 , 8 )])) For the example input tuple of (2, 2), the workflow will only select the first element (2) and run the task against that element. workflow = Workflow ([ Task ([ lambda x : [ y * 3 for y in x ], lambda x : [ y - 1 for y in x ]], unpack = False , column = { 0 : 0 , 1 : 1 })]) list ( workflow ([( 2 , 8 )])) The example above applies a separate action to each input column. This simple construct can help build extremely powerful workflow graphs!","title":"Extract task output columns"},{"location":"workflow/url/","text":"Url Task Task that processes urls. workflow = Workflow ([ UrlTask ()]) workflow ([ \"https://file.to.download\" , \"file:////local/file/to/copy\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Url"},{"location":"workflow/url/#url-task","text":"Task that processes urls. workflow = Workflow ([ UrlTask ()]) workflow ([ \"https://file.to.download\" , \"file:////local/file/to/copy\" ])","title":"Url Task"},{"location":"workflow/url/#txtai.workflow.task.base.UrlTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"},{"location":"workflow/workflow/","text":"Workflow Task Task that runs a Workflow. Allows creating workflows of workflows. workflow = Workflow ([ WorkflowTask ( workflow )]) workflow ([ \"input data\" ]) __init__ ( self , action = None , select = None , unpack = True , column = None , merge = 'hstack' , initialize = None , finalize = None , concurrency = None , ** kwargs ) special Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"Workflow"},{"location":"workflow/workflow/#workflow-task","text":"Task that runs a Workflow. Allows creating workflows of workflows. workflow = Workflow ([ WorkflowTask ( workflow )]) workflow ([ \"input data\" ])","title":"Workflow Task"},{"location":"workflow/workflow/#txtai.workflow.task.base.WorkflowTask.__init__","text":"Source code in txtai/workflow/task/base.py def __init__ ( self , action = None , select = None , unpack = True , column = None , merge = \"hstack\" , initialize = None , finalize = None , concurrency = None , ** kwargs ): \"\"\" Creates a new task. A task defines two methods, type of data it accepts and the action to execute for each data element. Action is a callable function or list of callable functions. Args: action: action(s) to execute on each data element select: filter(s) used to select data to process unpack: if data elements should be unpacked or unwrapped from (id, data, tag) tuples column: column index to select if element is a tuple, defaults to all merge: merge mode for joining multi-action outputs, defaults to hstack initialize: action to execute before processing finalize: action to execute after processing concurrency: sets concurrency method when execute instance available valid values: \"thread\" for thread-based concurrency, \"process\" for process-based concurrency kwargs: additional keyword arguments \"\"\" # Standardize into list of actions if not action : action = [] elif not isinstance ( action , list ): action = [ action ] self . action = action self . select = select self . unpack = unpack self . column = column self . merge = merge self . initialize = initialize self . finalize = finalize self . concurrency = concurrency # Check for custom registration. Adds additional instance members and validates required dependencies available. if hasattr ( self , \"register\" ): self . register ( ** kwargs ) elif kwargs : # Raise error if additional keyword arguments passed in without register method kwargs = \", \" . join ( f \"' { kw } '\" for kw in kwargs ) raise TypeError ( f \"__init__() got unexpected keyword arguments: { kwargs } \" )","title":"__init__()"}]}